{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud From Enron Email ML Project\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will play detective, and put my machine learning skills to use by building an algorithm to identify Enron Employees who may have committed fraud based on the public Enron financial and email dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Enron scandal, publicized in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure.\n",
    "\n",
    "The Enron fraud is a big, messy and totally fascinating story about corporate malfeasance of nearly every imaginable type. The Enron email and financial datasets are also big, messy treasure troves of information, which become much more useful once you know your way around them a bit. The email and finance data have been combined into a single dataset, which i will explore in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deal with an imperfect, real-world dataset\n",
    "* Validate a machine learning result using test data\n",
    "* Evaluate a machine learning result using quantitative metrics\n",
    "* Create, select and transform features\n",
    "* Compare the performance of machine learning algorithms\n",
    "* Tune machine learning algorithms for maximum performance\n",
    "* Communicate your machine learning algorithm results clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Dataset and Question\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>sizing</b> of the dataset used is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Population: \", len(data_dict)\n",
    "print \"Property Number: \", len(data_dict.values()[0])\n",
    "print \"Property Names:\", data_dict.values()[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Population:</b>  146<br>\n",
    "<b>Property Number:</b>  21<br>\n",
    "<b>Property Names:</b> ['salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'email_address', 'from_poi_to_this_person']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property <b>'poi'</b> is the classification label. It is not taken into account in the analysis of quality and selection of properties.<br>\n",
    "The <b>'email_address'</b> property is a string that is not useful for classification. It will not be taken into account in the following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To <b>measure the data quality</b> of the properties we checked the percentage of NaNs in each of the properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showNaNs(features, features_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN Percent\n",
    "34.7 salary\n",
    "43.8 bonus\n",
    "29.9 exercised_stock_options\n",
    "13.2 total_stock_value\n",
    "73.6 deferral_payments\n",
    "13.9 total_payments\n",
    "24.3 restricted_stock\n",
    "40.3 shared_receipt_with_poi\n",
    "88.2 restricted_stock_deferred\n",
    "34.7 expenses\n",
    "97.9 loan_advances\n",
    "39.9 to_messages\n",
    "40.3 from_messages\n",
    "36.1 other\n",
    "54.2 from_this_person_to_poi\n",
    "88.9 director_fees\n",
    "66.7 deferred_income\n",
    "54.9 long_term_incentive\n",
    "48.6 from_poi_to_this_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties with a low quality (NaN> 60) will not take them into account:\n",
    "<br>73.6 deferral_payments\n",
    "<br>88.2 restricted_stock_deferred\n",
    "<br>97.9 loan_advances\n",
    "<br>88.9 director_fees\n",
    "<br>66.7 deferred_income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point there are 14 properties to consider.\n",
    "<br>\n",
    "Using the following code to graphically show each property (2 at a time) together with the labeled POI / No POI analyzed if the properties are useful to discern the classification."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "showOnPlot(features, labels, feature_index_axis_x, feature_index_axis_y, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to finding an outliner (next point) we are keep the following properties (6) that it seems (after graphic analysis) are related to the POI / non-POI classification: <b>['salary', 'bonus', 'total_stock_value', 'total_payments','expenses', 'long_term_incentive']</b>\n",
    "\n",
    "We will also include properties related to messages sent / received with POIs but we will include them as percentages later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing graphically the selected properties a clear outliner is observed. Analyzing the dataset it is observed that it corresponds to <b>'TOTAL'</b> so we eliminate it from the dataset.\n",
    "\n",
    "In some cases other outliners seem to be observed, but they help us detect POIs so they do not have to be eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Feature Selection/Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a scaler to regularize all selected properties by regularizing them between the maximum and minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = MinMaxScaler().fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the properties 'from_this_person_to_poi', 'from_poi_to_this_person' and total from/to/shared messages passing them to percentages on the total of messages that correspond to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list.append('perc_poi_messages')\n",
    "include_perc_poi_messages(my_dataset, 'perc_poi_messages',\n",
    "                          ['from_this_person_to_poi', 'from_poi_to_this_person', 'shared_receipt_with_poi'],\n",
    "                          ['to_messages', 'from_messages', 'shared_receipt_with_poi'])\n",
    "\n",
    "features_list.append('perc_this_person_to_poi')\n",
    "include_perc_poi_messages(my_dataset, 'perc_this_person_to_poi', ['from_this_person_to_poi'], ['from_messages'])\n",
    "\n",
    "features_list.append('perc_poi_to_this_person')\n",
    "include_perc_poi_messages(my_dataset, 'perc_poi_to_this_person', ['from_poi_to_this_person'], ['to_messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I use PCA to accelerate the prediction speed of the algorithm. 8 is the minimum number of components before losing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=8)\n",
    "features = pca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick and Tune an Algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithms have been used for the POI detector:\n",
    "* Gaussian NB\n",
    "* SVC\n",
    "* Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = GaussianNB()\n",
    "#clf = svm.SVC(gamma=\"auto\", C=8000.0, kernel='rbf')\n",
    "#clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result was given by SVC and Naive Bayes.\n",
    "Finally SVC has been selected after choosing the best one using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10, 100, 1000, 3000, 5000, 8000, 9000, 10000, 11000]}\n",
    "clf = GridSearchCV(svm.SVC(gamma=\"auto\"), parameters, cv=5, iid=False, scoring='f1')\n",
    "clf.fit(features_train, labels_train)\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The iterated parameters have been 'kernel' and 'C' and I have set 'gamma'. As scoring I have sought to maximize f1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and Evaluate\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account that the dataset is very small, the results should not be very good.\n",
    "\n",
    "Next I show the results with the selected metrics and they are better than I expected at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Accuracy Train\", round(clf.score(features_train, labels_train), 2)\n",
    "print \"Accuracy Test\", round(clf.score(features_test, labels_test), 2)\n",
    "\n",
    "prediction_test = clf.predict(features_test)\n",
    "print \"Precision Score\", round(precision_score(labels_test, prediction_test, average='binary'), 2)\n",
    "print \"Recall Score\", round(recall_score(labels_test, prediction_test, average='binary'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy Train 0.98\n",
    "Accuracy Test 0.91\n",
    "Precision Score 0.75\n",
    "Recall Score 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project context (detection of culprits) I am interested in maximizing precision but without forgetting the rest of the parameters (accuracy, recall). So I chose a scoring f1 reviewing the parameters and properties to choose the configuration that generated high precision.<br>\n",
    "The result of a 75% precision seems very good (in this context of exercise) being only 25% of non-POI predicted as POI (false positives).<br>\n",
    "On the other hand only 50% (recall) of true POI will not be detected (false negative).<br>\n",
    "The general accuracy is very high 0.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[Wikipedia] https://en.wikipedia.org/wiki/Enron_scandal<br>\n",
    "[Scikit Learn Web] https://scikit-learn.org/stable/\n",
    "\n",
    "I hereby confirm that this submission is my work. I have cited above the origins of any parts of the submission that were taken from Websites, books, forums, blog posts, github repositories, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
