{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud From Enron Email ML Project\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will play detective, and put my machine learning skills to use by building an algorithm to identify Enron Employees who may have committed fraud based on the public Enron financial and email dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Enron scandal, publicized in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure.\n",
    "\n",
    "The Enron fraud is a big, messy and totally fascinating story about corporate malfeasance of nearly every imaginable type. The Enron email and financial datasets are also big, messy treasure troves of information, which become much more useful once you know your way around them a bit. The email and finance data have been combined into a single dataset, which i will explore in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deal with an imperfect, real-world dataset\n",
    "* Validate a machine learning result using test data\n",
    "* Evaluate a machine learning result using quantitative metrics\n",
    "* Create, select and transform features\n",
    "* Compare the performance of machine learning algorithms\n",
    "* Tune machine learning algorithms for maximum performance\n",
    "* Communicate your machine learning algorithm results clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Dataset and Question\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>sizing</b> of the dataset used is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Population: \", len(data_dict)\n",
    "print \"Property Number: \", len(data_dict.values()[0])\n",
    "print \"Property Names:\", data_dict.values()[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Population:</b>  146<br>\n",
    "<b>Property Number:</b>  21<br>\n",
    "<b>Property Names:</b> ['salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'email_address', 'from_poi_to_this_person']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property <b>'poi'</b> is the classification label. It is not taken into account in the analysis of quality and selection of properties.<br>\n",
    "The <b>'email_address'</b> property is a string that is not useful for classification. It will not be taken into account in the following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balance of POI/no POI (after removing the elements of population with all properties with NaNs value) is around <b>12.5% of POI (18 POI/125 no POI)</b>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "POI Balance: 12.59 ( 18 / 143 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To <b>measure the data quality</b> of the properties we checked the percentage of NaNs in each of the properties (after removing the elements of population with all properties with NaNs value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_utils_show_nans(features, features_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-a6a847d966ae>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-a6a847d966ae>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    NaN Percent\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NaN Percent\n",
    "34.7 salary\n",
    "43.8 bonus\n",
    "29.9 exercised_stock_options\n",
    "13.2 total_stock_value\n",
    "73.6 deferral_payments\n",
    "13.9 total_payments\n",
    "24.3 restricted_stock\n",
    "40.3 shared_receipt_with_poi\n",
    "88.2 restricted_stock_deferred\n",
    "34.7 expenses\n",
    "97.9 loan_advances\n",
    "39.9 to_messages\n",
    "40.3 from_messages\n",
    "36.1 other\n",
    "54.2 from_this_person_to_poi\n",
    "88.9 director_fees\n",
    "66.7 deferred_income\n",
    "54.9 long_term_incentive\n",
    "48.6 from_poi_to_this_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following function i check all the people names to try to find some error.\n",
    "I found <b>'THE TRAVEL AGENCY IN THE PARK'</b> to remove from the data because it is not a person to analize."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "poi_utils.show_person_names(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following code to graphically show each property (2 at a time) together with the labeled POI / No POI analyzed if there are any unexpected value."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "poi_utils.show_on_plot(features, labels, feature_index_axis_x, feature_index_axis_y, features_list)\n",
    "poi_utils.show_person_properties(dataset, prop_name, threshold, conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing graphically the selected properties a clear outliner is observed. Analyzing the dataset it is observed that it corresponds to <b>'TOTAL'</b> so we eliminate it from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also remove the following because:\n",
    "<br><b>'BHATNAGAR SANJAY'</b>: Wrong data on 'exercised_stock_options' and 'restricted_stock' (data exchange)\n",
    "<br><b>'BELFER ROBERT'</b>: 'restricted_stock_deferred' must be negative\n",
    "<br><b>'HAUG DAVID L'</b>: Total Payment 475. No other payment data and only stock value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases other outliners seem to be observed, but they help us detect POIs so they do not have to be eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Feature Selection/Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a scaler to regularize all selected properties by regularizing them between the maximum and minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the properties 'from_this_person_to_poi', 'from_poi_to_this_person' and total from/to/shared messages passing them to percentages on the total of messages that correspond to them.\n",
    "I also include:\n",
    "'bonus_salary'. Proportion of bonus on salary.\n",
    "'incentive_salary'. Proportion of 'long_term_incentive' on salary\n",
    "'stock_payment'. Proportion of 'total_stock_value' on 'total_payments'\n",
    "'total'. Aggregation of 'total_payments' and 'total_stock_value'\n",
    "'total_salary'. Proportion of 'total' on salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_utils.include_div_poi(my_dataset, features_list, 'perc_poi_messages',\n",
    "                          ['from_this_person_to_poi', 'from_poi_to_this_person', 'shared_receipt_with_poi'],\n",
    "                          ['to_messages', 'from_messages', 'shared_receipt_with_poi'])\n",
    "poi_utils.include_div_poi(my_dataset, features_list, 'perc_this_person_to_poi', ['from_this_person_to_poi'], ['from_messages'])\n",
    "poi_utils.include_div_poi(my_dataset, features_list, 'perc_poi_to_this_person', ['from_poi_to_this_person'], ['to_messages'])\n",
    "poi_utils.include_div_poi(my_dataset, features_list, 'bonus_salary', ['bonus'], ['salary'])\n",
    "poi_utils.include_div_poi(my_dataset, features_list, 'incentive_salary', ['long_term_incentive'], ['salary'])\n",
    "poi_utils.include_div_poi(my_dataset, features_list, 'stock_payment', ['total_stock_value'], ['total_payments'])\n",
    "poi_utils.include_add_poi(my_dataset, features_list, 'total', ['total_payments', 'total_stock_value'])\n",
    "poi_utils.include_div_poi(my_dataset, features_list, 'total_salary', ['total'], ['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a scaler, selector and PCA in a pipeline on the training and predictions in order to fast the algorithm.\n",
    "A scaler (MinMaxScaler) in order to regularizing them between the maximum and minimum.\n",
    "A selector (SelectKBest) in order to select the most significant features.\n",
    "A PCA in order to reduce to the main component of the selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick and Tune an Algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithms have been used for the POI detector:\n",
    "* Gaussian NB\n",
    "* KNeighbors Classifier\n",
    "* Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the parameters used in the tuning of the scaler, selector, reducer and classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB\n",
    "pipe_nbc = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('selector', SelectKBest()),\n",
    "        ('reducer', PCA()),\n",
    "        ('classifier', GaussianNB())\n",
    "    ])\n",
    "\n",
    "param_grid_nbc = {\n",
    "    'scaler':                [None, MinMaxScaler()],\n",
    "    'selector__k':           [10, 11, 12, 14, 16, 'all'],\n",
    "    'reducer__n_components': [5, 6, 7, 8],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighbors Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the parameters used in the tuning of the scaler, selector, reducer and classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNeighborsClassifier\n",
    "pipe_knc = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('selector', SelectKBest()),\n",
    "        ('reducer', PCA()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "param_grid_knc = {\n",
    "    'scaler':                  [None, MinMaxScaler()],\n",
    "    'selector__k':             [10, 11, 12, 14, 15, 18, 'all'],\n",
    "    'reducer__n_components':   [5, 6, 7, 8, 10],\n",
    "    'classifier__n_neighbors': [2, 3, 4, 5, 6],\n",
    "    'classifier__weights':     ['uniform', 'distance'],\n",
    "    'classifier__algorithm':   ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'classifier__leaf_size':   [20, 30, 40],\n",
    "    'classifier__p':           [1, 2, 3],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the parameters used in the tuning of the scaler, selector, reducer and classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "pipe_dtc = Pipeline([\n",
    "        ('scaler', None),\n",
    "        ('selector', SelectKBest()),\n",
    "        ('reducer', PCA()),\n",
    "        ('classifier', DecisionTreeClassifier())\n",
    "    ])\n",
    "\n",
    "param_grid_dtc = {\n",
    "    'scaler':                        [None, MinMaxScaler()],\n",
    "    'selector__k':                   [10, 11, 12, 14, 16, 'all'],\n",
    "    'reducer__n_components':         [5, 6, 7, 8],\n",
    "    'classifier__criterion':         ['gini', 'entropy'],\n",
    "    'classifier__splitter':          ['best', 'random'],\n",
    "    'classifier__min_samples_split': [2, 3, 4, 5],\n",
    "    'classifier__class_weight':      ['balanced', None],\n",
    "    'classifier__min_samples_leaf':  [1, 2, 3, 4],\n",
    "    'classifier__max_depth':         [None, 5, 10, 20],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use GridSearchCV to automatically select the best parameters that adapt to the algorithm to get the best score.\n",
    "<br>I use StratifiedShuffleSplit for cross validation to get the best out of the few data we have by selecting training data set and test in the search for the best algorithm\n",
    "<br>I use f1 scoring in this case to maximize precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=42)\n",
    "grid = GridSearchCV(pipe, param_grid, scoring='f1', cv=sss, verbose=1, n_jobs=2)\n",
    "grid = grid.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Param/Score |    Gaussian NB    |    KNeighbors      |    Decission Tree       |\n",
    "|-------------|-------------------|--------------------|-------------------------|\n",
    "|Scaler       | MinMaxScaler()    | None               | MinMaxScaler()          |\n",
    "|Selector     | k = 12            | k = all            | k = 14                  |\n",
    "|Reducer      | n_components = 6  | n_components = 9   | n_components = 5        |\n",
    "|Classifer    | N/A               | n_neighbor = 2     | criterion = gini        |\n",
    "|.            |.                  | weights = distance | splitter = random       |\n",
    "|.            |.                  | algorithm = auto   | min_samples_split = 3   |\n",
    "|.            |.                  | leaf_size = 2      | class_weight = balanced |\n",
    "|.            |.                  | p =                | min_samples_leaf = 1    |\n",
    "|.            |.                  |.                   | max_depth = 5           |\n",
    "|.            |.                  |.                   |.                        |\n",
    "|Accuracy     | 0.82              | 0.79               | 0.69                    |\n",
    "|Precision    | 0.36              | 0.24               | 0.23                    |\n",
    "|Recall       | 0.30              | 0.21               | 0.48                    |\n",
    "|F1           | 0.33              | 0.22               | 0.31                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result obtained has been with a Pipeline with MinMaxScaler, SelectKBest(k=12), PCA(n_components=6) and <b>Gaussian NB</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and Evaluate\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation of the algorithm is a fundamental part. It is necessary to ensure that the results in their precisions are adapted to our expectations.<br>\n",
    "In this case, accuracy does not matter. It is necessary to ensure that precision and recall have significant values.<br>\n",
    "It is also necessary to assess the algorithm's performance of the speed at which we need the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account that the dataset is very small, the results should not be very good.\n",
    "\n",
    "Next I show the results with the selected metrics and they are better than I expected at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Accuracy Train\", round(grid.score(features_train, labels_train), 2)\n",
    "print \"Accuracy Test\", round(grid.score(features_test, labels_test), 2)\n",
    "\n",
    "prediction_test = grid.predict(features_test)\n",
    "print \"Precision Score\", round(precision_score(labels_test, prediction_test, average='binary'), 2)\n",
    "print \"Recall Score\", round(recall_score(labels_test, prediction_test, average='binary'), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project context (detection of fraud) I am interested in maximizing precision but without forgetting the rest of the parameters (accuracy, recall). So I chose a scoring f1 reviewing the parameters and properties to choose the configuration that generated high precision.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[Wikipedia] https://en.wikipedia.org/wiki/Enron_scandal<br>\n",
    "[Scikit Learn Web] https://scikit-learn.org/stable/<br>\n",
    "https://www.quora.com/How-do-I-properly-use-SelectKBest-GridSearchCV-and-cross-validation-in-the-sklearn-package-together<br>\n",
    "http://busigence.com/blog/hyperparameter-optimization-and-why-is-it-important<br>\n",
    "\n",
    "I hereby confirm that this submission is my work. I have cited above the origins of any parts of the submission that were taken from Websites, books, forums, blog posts, github repositories, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
